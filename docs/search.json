[
  {
    "objectID": "posts/transformers/index.html",
    "href": "posts/transformers/index.html",
    "title": "Transformers",
    "section": "",
    "text": "Transformers are the most popular NLP model architecture, and have been shown to achieve state-of-the-art results on a variety of NLP tasks. Transformers were introduced in the paper Attention is All You Need and have since become the standard for NLP tasks. I have been interested in learning more about transformers and how they work, so I decided to implement a transformer model from scratch. In this notebook, I will walk through the process of implementing a transformer model for the task of text generation. This notebook is inspired by Andrej Karpathy’s Let’s build GPT video."
  },
  {
    "objectID": "posts/transformers/index.html#tokenization",
    "href": "posts/transformers/index.html#tokenization",
    "title": "Transformers",
    "section": "1.1 Tokenization",
    "text": "1.1 Tokenization\nThe text we have is in the form of a string, but we need to convert them to numbers so that we can feed them into our model. This process is called tokenization. To tokenize a text we split a text to its smallest meaningful units, which are called tokens.\nThere are many different ways to tokenize text, the simplest form being splitting the text into individual letters known as character-level tokenization. This has a very small vocabulary size (number of unique tokens), for example if we are tokenizing lower case english text, the vocabulary size would be 26 (26 letters in the alphabet). However, this is not very useful for our purposes, as we want to capture the meaning of the text. Since we are starting with letters the model has to learn the meaning of words from scratch, which is not very efficient.\nAnother way would be to tokenize the text into words, which is known as word-level tokenization. This has a much larger vocabulary size, since english language has a lot of unique words. Dropping least frequent words from the vocabulary and replacing them with a special token like [UNK] is one way to reduce the vocabulary size.\nSub-word tokenization is a middle ground between character-level and word-level tokenization. In sub-word tokenization, we split the text into sub-words, which are smaller than words but larger than characters. This allows us to capture the meaning of words while keeping the vocabulary size manageable. Most of the popular transformer models use sub-word tokenization. In this notebook, we will use the Wordpiece tokenizer from the HuggingFace tokenizers library for the sake of simplicity\n\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.trainers import WordLevelTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\n\ntokenizer_word = Tokenizer(WordLevel(unk_token='[UNK]'))\ntokenizer_word.pre_tokenizer = Whitespace() \ntrainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"],\n1                           min_frequency=2)\ntokenizer_word.train_from_iterator(subtitle_list,trainer=trainer)\n\n\n1\n\nThe special tokens are added to the vocabulary to represent Unknown tokens, padding tokens, start of sequence tokens, end of sequence tokens respectively. We also drop words which appear only once in the text to reduce the vocabulary size.\n\n\n\n\nIf we check the vocabulary size, we can see that it is 12731\n\ntokenizer_word.get_vocab_size()\n\n12731\n\n\nAfter training the tokenizer, we can use it to tokenize the text adding the special tokens to the start and end of the each text.\n\neos = [tokenizer_word.token_to_id(\"[EOS]\")]\nsos = [tokenizer_word.token_to_id(\"[SOS]\")]\n\ndata = []\n\nfor sub in subtitle_list:\n  data+=(sos+ tokenizer_word.encode(sub).ids +eos)"
  },
  {
    "objectID": "posts/transformers/index.html#embedding-layer",
    "href": "posts/transformers/index.html#embedding-layer",
    "title": "Transformers",
    "section": "2.1 Embedding Layer",
    "text": "2.1 Embedding Layer\nThe embedding layer is the first layer in the transformer model. It takes the input tokens and converts them to a vector representation. The embedding layer is a trainable layer, which means that the embedding vectors are learned during training. Each token is represented by a vector of size \\(d_{model}\\). The embedding layer is implemented in pytorch as nn.Embedding, it is like a lookup table which maps the token ids to embedding vectors. After converting the tokens to embedding vectors, we add the positional encodings to the embedding vectors. The positional encodings are added to the embedding vectors to give the model information about the position of the tokens in the sequence. In the paper, they use sine and cosine functions to generate the positional encodings given by the following equations:\n\\[PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}})\\] \\[PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}})\\]\nwhere \\(pos\\) is the position, \\(i\\) is the dimension and \\(d_{model}\\) is the embedding dimension. For sake of simplicity, we will just use the indices of the position as the positional encodings.\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nvocab_size = tokenizer_word.get_vocab_size()\nseq_len = 128\nd_model = 512\n\n1device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nclass TextGenerationModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n2        self.token_embedding_table = nn.Embedding(vocab_size, d_model)\n3        self.position_embedding_table = nn.Embedding(seq_len, d_model)\n\n    def forward(self, x):\n        tok_emb = self.token_embedding_table(x) # shape -&gt; (batch_size,seq_len,d_model)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # shape -&gt; (seq_len,d_model)\n        x = tok_emb + pos_emb # shape -&gt; (batch_size,seq_len,d_model)\n\n\n1\n\ndevice variable is set to cuda if we have access to a GPU, else it is set to cpu. This is done so that we can move the tensors to the appropriate device.\n\n2\n\ntoken_embedding_table is of shape (vocab_size, d_model) - each row represents the embedding vector of size d_model for a token in the vocabulary.\n\n3\n\npositional_encoding_table is of shape (seq_len, d_model) - each row represents the positional encoding vector of size d_model for a position in the sequence."
  },
  {
    "objectID": "posts/transformers/index.html#multi-head-attention",
    "href": "posts/transformers/index.html#multi-head-attention",
    "title": "Transformers",
    "section": "2.2 Multi-Head Attention",
    "text": "2.2 Multi-Head Attention\nPrevious NLP architectures like RNNs had only one hidden state, which had to encode all the information about the input sequence. This made it difficult for the model to learn long range dependencies. The attention mechanism solves this problem by allowing the model to focus on the relevant parts of the input sequence. The attention mechanism is a weighted sum of the values, where the weights are calculated by taking the dot product of the query and the key vectors. The query, key and value vectors are obtained by multiplying the input with three different weight matrices. The formula for attention is given by:\n\\[Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V\\]\nwhere \\(Q\\) is the query matrix, \\(K\\) is the key matrix, \\(V\\) is the value matrix and \\(d_k\\) is the dimension of the key matrix. Intuitively, we can assume the query matrix as the list items we wish to purchase in a store, the key matrix as the list of items in the store and the value as the actual items we purchase. When we take a dot product of the query and key matrix, we get a score for each item in the store. The higher the score the more relevant the items is to our query. The softmax function is applied to the scores to get a probability distribution over the items in the store. The value matrix is multiplied with the probability distribution to get the quantity of each items we wish to purchase.\nThere maybe more than one attention mechanism in a transformer model, each of which is called a head. The multi-head attention mechanism is a concatenation of multiple attention heads. The decoder uses masked multi-head attention, which is similar to multi-head attention except that the attention weights are masked so that the model cannot attend to future tokens i.e when predicting the next token, the model can only attend to the tokens that have already been predicted. The masked multi-head attention can be implemented by this cool trick explained by Andrej Karpathy in his video.\n\nseq_len,d_model = 4,2\nx = torch.randn(seq_len,d_model)\nx \n\ntensor([[-2.0260, -2.0655],\n        [-1.2054, -0.9122],\n        [-1.2502,  0.8032],\n        [-0.2071,  0.0544]])\n\n\n\n1tril = torch.tril(torch.ones(seq_len, seq_len))\nwei = torch.zeros((seq_len,seq_len))\n2wei = wei.masked_fill(tril == 0, float('-inf'))\nwei\n\n\n1\n\nLower triangular matrix of shape (seq_len, seq_len) with ones on and below the diagonal and zeros above the diagonal.\n\n2\n\nMask weights is implemented zeros on and below the diagonal and negative infinity above the diagonal.\n\n\n\n\ntensor([[0., -inf, -inf, -inf],\n        [0., 0., -inf, -inf],\n        [0., 0., 0., -inf],\n        [0., 0., 0., 0.]])\n\n\n\n3wei = F.softmax(wei, dim=-1)\nwei\n\n\n3\n\nSoftmax function applied to the masked weights will be zero for the tokens above the diagonal.\n\n\n\n\ntensor([[1.0000, 0.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000, 0.0000],\n        [0.3333, 0.3333, 0.3333, 0.0000],\n        [0.2500, 0.2500, 0.2500, 0.2500]])\n\n\n\n4wei@x\n\n\n4\n\nThe masked weights are multiplied with the attention weights to mask the future tokens. We can see that each row of the resultant matrix is the average of rows till that row.\n\n\n\n\ntensor([[-2.0260, -2.0655],\n        [-1.6157, -1.4889],\n        [-1.4939, -0.7248],\n        [-1.1722, -0.5300]])\n\n\nPutting it all together, we can implement the masked multi-head attention as follows:\n\ndropout = 0.1\n\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(d_model, head_size, bias=False) \n        self.query = nn.Linear(d_model, head_size, bias=False)\n        self.value = nn.Linear(d_model, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(seq_len, seq_len))) # To store the lower triangular matrix within the model itself\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        batch_size,seq_len,d_model = x.shape\n        k = self.key(x)   # (batch_size,seq_len,d_model)\n        q = self.query(x) # (batch_size,seq_len,d_model)\n\n        wei = q @ k.transpose(-2,-1) * d_model**-0.5 # (batch_size,seq_len,d_model) @ (batch_size,d_model,seq_len) -&gt; (batch_size,seq_len,seq_len)\n        wei = wei.masked_fill(self.tril[:seq_len, :seq_len] == 0, float('-inf')) # (batch_size,seq_len,seq_len)\n        wei = F.softmax(wei, dim=-1) # (batch_size,seq_len,seq_len)\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (batch_size,seq_len,d_model)\n        out = wei @ v # (batch_size,seq_len,seq_len) @ (batch_size,seq_len,d_model) -&gt; (batch_size,seq_len,d_model)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(d_model, d_model) # Feed-forward layer \n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out"
  },
  {
    "objectID": "posts/transformers/index.html#feed-forward-layer",
    "href": "posts/transformers/index.html#feed-forward-layer",
    "title": "Transformers",
    "section": "2.3 Feed Forward Layer",
    "text": "2.3 Feed Forward Layer\nWe have learned the embeddings of the next token using the embeddings of the input tokens, but we need to convert this into probabilities over the vocabulary. This is done using the feed forward layer followed by a softmax. The feed forward layer is a simple two layer neural network with a ReLU activation function in between.\n\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, d_model):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(d_model, 4 * d_model),\n            nn.ReLU(),\n            nn.Linear(4 * d_model, d_model),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)"
  },
  {
    "objectID": "posts/transformers/index.html#transformer-model",
    "href": "posts/transformers/index.html#transformer-model",
    "title": "Transformers",
    "section": "2.4 Transformer Model",
    "text": "2.4 Transformer Model\nNow that we have all the layers implemented, we can put them together to create the transformer block. The transformer block consists of the Multi-Head Attention layer with n_head heads, followed by a feed forward layer. Before passing the inputs through these layers, we add a layer normalization layer. There are also skip connections around the multi-head attention and feed forward layers.\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, d_model, n_head):\n        # n_head: the number of heads we'd like\n        super().__init__()\n        head_size = d_model // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(d_model)\n        self.ln1 = nn.LayerNorm(d_model)\n        self.ln2 = nn.LayerNorm(d_model)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\nNow adding our transformer block to our text generation model, we create the token embeddings for the input tokens and add the positional encodings. We passed the embedded inputs into sequence of transformer blocks of length n_layer. The output of the transformer blocks is passed through another layer norm followed by linear layer, which is used to convert the predicted embeddings into probabilities over the vocabulary.\nWe also implement the generate function, which starts with the start token and predicts the next token, this is done recursively with newly predicted tokens being added to the input sequence until we reach the maximum sequence length.\n\nn_head = 6 # Number of heads\nn_layer = 8 # Number of decoder layers in the transformer\n\nclass TextGenerationModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, d_model)\n        self.position_embedding_table = nn.Embedding(seq_len, d_model)\n        self.blocks = nn.Sequential(*[Block(d_model, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(d_model) # final layer norm\n        self.lm_head = nn.Linear(d_model, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # shape -&gt; (batch_size,seq_len,d_model)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # shape -&gt; (seq_len,d_model)\n        x = tok_emb + pos_emb  # shape -&gt; (batch_size,seq_len,d_model)\n        x = self.blocks(x)  # shape -&gt; (batch_size,seq_len,d_model)\n        x = self.ln_f(x)  # shape -&gt; (batch_size,seq_len,d_model)\n        logits = self.lm_head(x) # shape -&gt; (batch_size,seq_len,vocab_size)\n\n1        if targets is None:\n            loss = None\n        else:\n            batch_size,seq_len,d_model = logits.shape\n            logits = logits.view(batch_size*seq_len, d_model)\n            targets = targets.view(batch_size*seq_len)\n2            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n3            idx_cond = idx[:, -seq_len:]\n            logits, loss = self(idx_cond)\n            logits = logits[:, -1, :] # (batch_size, vocab_size) - only get the last timestep's logits\n            probs = F.softmax(logits, dim=-1) \n4            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1) # (batch_size, vocab_size+1) - append the new token to the end of the sequence\n        return idx\n\n\n1\n\nDuring validation we do not have the targets, we return the loss as None\n\n2\n\nDuring training, we calculate the cross entropy loss between the predicted tokens and the target tokens.\n\n3\n\nWe get the last seq_len tokens from the predicted tokens to use as the input for the next iteration.\n\n4\n\nThe next token is predicted using the probabilities over the vocabulary."
  },
  {
    "objectID": "posts/transformers/index.html#training",
    "href": "posts/transformers/index.html#training",
    "title": "Transformers",
    "section": "2.5 Training",
    "text": "2.5 Training\nWe split our data after converting the tokens to tensor in 90:10 ratio for training and validation.\n\nimport torch\ndata = torch.tensor(data)\n\nn = int(0.9*len(data)) \ntrain_data = data[:n]\nval_data = data[n:]\n\nWe now have to create a dataloader to feed the data to our model. The get_batch function returns the input tokens and the target tokens. The input tokens are of length seq_len and the target tokens is the immediate next token for each input token. Since training over single inputs in not efficient we do it batch of size - batch_size.\nFor example if out sequence is [1,2,3,4,5,6,7,8,9,10] and seq_len is 3, then the input tokens and target tokens will be as follows:\nx = [1,2,3] y = [2,3,4]\nFor each index i in x the input is x[0:i+1] and the target is y[i], i.e for i = 0 the input is [1] and the target is 2, for i = 1 the input is [1,2] and the target is 3 and so on.\n\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - seq_len, (batch_size,))\n    x = torch.stack([data[i:i+seq_len] for i in ix])\n    y = torch.stack([data[i+1:i+seq_len+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\nThe estimate_loss function returns the train and test loss aggregated over number of iterations (eval_iters), this function is called at regular intervals to check the progress of the model during training.\n\neval_iters = 100\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\nWe use the AdamW optimizer with a learning rate of 0.0001. We train the model for max_iters iterations and log the loss at ever 100 iterations.\n\nmax_iters = 5000\neval_interval = 100\n\nmodel = TextGenerationModel()\nm = model.to(device)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()"
  },
  {
    "objectID": "posts/transformers/index.html#references",
    "href": "posts/transformers/index.html#references",
    "title": "Transformers",
    "section": "3.1 References",
    "text": "3.1 References\n\nAttention is All You Need\nLet’s build GPT by Andrej Karpathy\nUmar Jamil’s transformer implementation\n\n\n\n\n\n\n\nNote\n\n\n\nThis was just a learning exercise for me, I am not an expert in NLP or transformers. If you find any mistakes or have any suggestions, please let me know."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Transformers\n\n\n\n\n\n\nnlp\n\n\n\nImplementing Transformers from scratch for text generation\n\n\n\n\n\nJan 5, 2024\n\n\nKarthi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi I am Karthi. Welcome to my blog. I am interested in a lot of things, but the main topics I will mostly write about is my ML learnings. I am currently working as a software engineer in a Big Tech company. If you would like to connect with me, you can find me on the below platforms."
  }
]